# -*- coding: utf-8 -*-
"""
Liberoç»Ÿä¸€æ•°æ®è½¬æ¢å™¨

æ”¯æŒè‡ªåŠ¨è¯†åˆ«RLDSå’ŒHDF5æ ¼å¼ï¼Œè‡ªåŠ¨è§£ææˆLeRobotDataSetæ ¼å¼ï¼Œæ”¯æŒå¤šçº¿ç¨‹æ“ä½œ
"""

import argparse
import ast
import json
import logging
import shutil
import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Tuple
import concurrent.futures
from functools import partial

import cv2
import h5py
import numpy as np
from tqdm import tqdm

# æ£€æŸ¥ä¾èµ–
try:
    import tensorflow_datasets as tfds
    import tensorflow as tf
    HAS_TF = True
except ImportError:
    HAS_TF = False
    logging.warning("tensorflow_datasetsæœªå®‰è£…ï¼ŒRLDSæ”¯æŒå°†è¢«ç¦ç”¨")

from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from lerobot.common.constants import HF_LEROBOT_HOME

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetFormatDetector:
    """æ•°æ®é›†æ ¼å¼æ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_format(data_path: Union[str, Path]) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æ ¼å¼
        
        Args:
            data_path: æ•°æ®é›†è·¯å¾„
            
        Returns:
            str: 'rlds' æˆ– 'hdf5'
        """
        data_path = Path(data_path)
        
        # æ£€æŸ¥HDF5æ ¼å¼ï¼šæŸ¥æ‰¾.hdf5æˆ–.h5æ–‡ä»¶
        hdf5_files = list(data_path.rglob("*.hdf5")) + list(data_path.rglob("*.h5"))
        
        # æ£€æŸ¥RLDSæ ¼å¼ï¼šæŸ¥æ‰¾tfrecordæ–‡ä»¶æˆ–dataset_info.json
        rlds_indicators = (
            list(data_path.rglob("*.tfrecord*")) + 
            list(data_path.rglob("dataset_info.json")) +
            list(data_path.rglob("features.json"))
        )
        
        if hdf5_files and not rlds_indicators:
            logger.info(f"æ£€æµ‹åˆ°HDF5æ ¼å¼ï¼Œæ‰¾åˆ°{len(hdf5_files)}ä¸ªHDF5æ–‡ä»¶")
            return "hdf5"
        elif rlds_indicators and not hdf5_files:
            logger.info(f"æ£€æµ‹åˆ°RLDSæ ¼å¼ï¼Œæ‰¾åˆ°ç›¸å…³æ–‡ä»¶ï¼š"
                       f"{[f.name for f in rlds_indicators[:3]]}")
            return "rlds"
        elif hdf5_files and rlds_indicators:
            logger.warning("åŒæ—¶å‘ç°HDF5å’ŒRLDSæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨HDF5æ ¼å¼")
            return "hdf5"
        else:
            raise ValueError(f"æ— æ³•æ£€æµ‹æ•°æ®æ ¼å¼ï¼š{data_path}")


class HDF5Processor:
    """HDF5æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, image_size: Tuple[int, int] = (256, 256), 
                 use_videos: bool = False):
        """
        åˆå§‹åŒ–HDF5å¤„ç†å™¨
        
        Args:
            image_size: å›¾åƒå°ºå¯¸ (height, width) - åŒ¹é…numpyæ•°ç»„æ ¼å¼
            use_videos: æ˜¯å¦ä½¿ç”¨è§†é¢‘æ ¼å¼
        """
        self.image_size = image_size  # (height, width)
        self.use_videos = use_videos
    
    def get_default_features(self, use_videos: bool = True) -> Dict[str, Dict[str, Any]]:
        """è·å–Liberoæ•°æ®é›†çš„é»˜è®¤ç‰¹å¾é…ç½®"""
        image_dtype = "video" if use_videos else "image"
        
        return {
            "observation.images.front": {
                "dtype": image_dtype,
                "shape": (*self.image_size, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.images.wrist": {
                "dtype": image_dtype,
                "shape": (*self.image_size, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.state": {
                "dtype": "float32",
                "shape": (7,),
                "names": [f"state_{i}" for i in range(7)],
            },
            "action": {
                "dtype": "float32",
                "shape": (7,),
                "names": [f"action_{i}" for i in range(7)],
            }
        }
    
    def process_episode(self, episode_path: Path, dataset: LeRobotDataset, task_name: str) -> bool:
        """
        å¤„ç†å•ä¸ªepisodeæ•°æ®
        
        Args:
            episode_path: episodeæ–‡ä»¶è·¯å¾„
            dataset: LeRobotæ•°æ®é›†
            task_name: ä»»åŠ¡åç§°
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            with h5py.File(episode_path, "r") as file:
                logger.debug(f"HDF5æ–‡ä»¶é”®: {list(file.keys())}")
                
                # æ£€æµ‹HDF5æ–‡ä»¶æ ¼å¼ï¼Œæ”¯æŒå¤šç§Liberoæ ¼å¼
                if "data" in file:
                    # æ–°çš„Liberoæ ¼å¼ï¼šdata/demo_N/...
                    return self._process_libero_demo_format(file, dataset, task_name, episode_path)
                else:
                    logger.warning(f"æœªè¯†åˆ«çš„HDF5æ ¼å¼: {episode_path}")
                    return False

        except (FileNotFoundError, OSError, KeyError) as e:
            logger.error(f"è·³è¿‡ {episode_path}: {str(e)}")
            return False
            

    def _process_libero_demo_format(self, file: h5py.File, dataset: LeRobotDataset, task_name: str, file_path: Optional[Path] = None) -> bool:
        """å¤„ç†æ–°çš„Libero demoæ ¼å¼ï¼šdata/demo_N/..."""
        data_group = file["data"]
        
        # å°è¯•ä»æ–‡ä»¶æ ¹çº§åˆ«æå–ä»»åŠ¡ä¿¡æ¯
        task_str = self._extract_task_info(file, task_name, file_path)
        
        # è·å–æ‰€æœ‰demo
        demo_keys = [k for k in data_group.keys() if k.startswith("demo_")]
        demo_keys.sort(key=lambda x: int(x.split("_")[1]))  # æŒ‰æ•°å­—æ’åº
        
        for demo_key in demo_keys:
            demo_group = data_group[demo_key]
            logger.info(f"å¤„ç† {demo_key}")
            
            # å°è¯•ä»demoçº§åˆ«æå–ä»»åŠ¡ä¿¡æ¯
            demo_task_str = self._extract_task_info(demo_group, task_str, file_path)
            
            # è¯»å–åŠ¨ä½œæ•°æ®
            actions = np.array(demo_group["actions"])
            
            # è¯»å–è§‚å¯Ÿæ•°æ®
            obs_group = demo_group["obs"]
            
            # è¯»å–å…³èŠ‚çŠ¶æ€ - ä½œä¸ºobservation.state
            joint_states = np.array(obs_group["joint_states"])
            
            # è¯»å–å›¾åƒæ•°æ®
            agentview_rgb = np.array(obs_group["agentview_rgb"])  # å‰è§†å›¾åƒ
            eye_in_hand_rgb = np.array(obs_group["eye_in_hand_rgb"])  # è…•éƒ¨å›¾åƒ
            
            # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
            num_frames = min(len(actions), len(joint_states), len(agentview_rgb), len(eye_in_hand_rgb))
            
            # å¤„ç†æ¯ä¸€å¸§
            for i in tqdm(range(num_frames), desc=f"å¤„ç† {demo_key}", leave=False):
                # å¤„ç†å›¾åƒï¼šè°ƒæ•´å¤§å°åˆ°ç›®æ ‡å°ºå¯¸
                front_img = cv2.resize(agentview_rgb[i], (self.image_size[1], self.image_size[0]))
                wrist_img = cv2.resize(eye_in_hand_rgb[i], (self.image_size[1], self.image_size[0]))
                
                # ä¿®å¤å›¾åƒæ—‹è½¬é—®é¢˜ï¼šç¿»è½¬180åº¦
                front_img = cv2.flip(front_img, -1)  # -1è¡¨ç¤º180åº¦ç¿»è½¬
                wrist_img = cv2.flip(wrist_img, -1)  # -1è¡¨ç¤º180åº¦ç¿»è½¬
                
                # å‡†å¤‡å¸§æ•°æ® - å‚è€ƒRLDSæ ¼å¼
                frame_data = {
                    "task": demo_task_str,
                    "action": actions[i].astype(np.float32),
                    "observation.state": joint_states[i].astype(np.float32),
                    "observation.images.front": front_img,
                    "observation.images.wrist": wrist_img,
                }
                
                # æ·»åŠ å¸§åˆ°æ•°æ®é›† - ä¿®å¤APIè°ƒç”¨
                dataset.add_frame(frame_data)
            
            # æ¯ä¸ªdemoä¿å­˜ä¸ºä¸€ä¸ªepisode
            dataset.save_episode()
        
        return True

    def _extract_task_info(self, group: h5py.Group, default_task: str, file_path: Optional[Path] = None) -> str:
        """ä»HDF5ç»„ä¸­æå–ä»»åŠ¡ä¿¡æ¯"""
        # å°è¯•å¤šç§å¯èƒ½çš„ä»»åŠ¡ä¿¡æ¯å­—æ®µ
        task_fields = [
            "language_instruction", "task_description", "task", "description",
            "instruction", "goal", "task_name", "task_info"
        ]
        
        for field in task_fields:
            if field in group:
                try:
                    task_data = group[field]
                    if hasattr(task_data, 'asstr'):
                        # å¤„ç†å­—ç¬¦ä¸²æ•°ç»„
                        task_str = task_data.asstr()[()]
                    elif isinstance(task_data, (bytes, str)):
                        # å¤„ç†å­—èŠ‚æˆ–å­—ç¬¦ä¸²
                        task_str = task_data.decode() if isinstance(task_data, bytes) else task_data
                    else:
                        # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        task_str = str(task_data[()])
                    
                    if task_str and task_str.strip():
                        logger.info(f"ä»å­—æ®µ '{field}' æå–åˆ°ä»»åŠ¡: {task_str}")
                        return task_str.strip()
                except Exception as e:
                    logger.debug(f"æå–å­—æ®µ '{field}' å¤±è´¥: {e}")
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡ä¿¡æ¯ï¼Œå°è¯•ä»æ–‡ä»¶åæå–
        if file_path is not None:
            task_str = self._extract_task_from_filename(file_path)
            if task_str:
                logger.info(f"ä»æ–‡ä»¶åæå–åˆ°ä»»åŠ¡: {task_str}")
                return task_str
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡ä¿¡æ¯ï¼Œè¿”å›é»˜è®¤å€¼
        return default_task
    
    def _extract_task_from_filename(self, file_path: Path) -> Optional[str]:
        """ä»æ–‡ä»¶åä¸­æå–ä»»åŠ¡æè¿°"""
        try:
            # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            filename = file_path.stem
            
            # å¸¸è§çš„åç¼€éœ€è¦ç§»é™¤
            suffixes_to_remove = [
                '_demo', '_trajectory', '_episode', '_data', '_hdf5',
                'demo', 'trajectory', 'episode', 'data', 'hdf5'
            ]
            
            # ç§»é™¤åç¼€
            task_name = filename
            for suffix in suffixes_to_remove:
                if task_name.endswith(suffix):
                    task_name = task_name[:-len(suffix)]
                    break
            
            # å°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä½¿å…¶æ›´æ˜“è¯»
            task_name = task_name.replace('_', ' ')
            
            # å¦‚æœä»»åŠ¡åç§°å¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ä»»åŠ¡æè¿°
            if len(task_name.strip()) < 5:
                return None
            
            return task_name.strip()
            
        except Exception as e:
            logger.debug(f"ä»æ–‡ä»¶åæå–ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def _extract_libero_demo_frames(self, file: h5py.File, task_name: str, file_path: Optional[Path] = None) -> List[List[Dict]]:
        """æå–Libero demoæ ¼å¼çš„å¸§æ•°æ®ï¼ˆç”¨äºå¤šçº¿ç¨‹å¤„ç†ï¼‰- è¿”å›æŒ‰demoåˆ†ç»„çš„æ•°æ®"""
        demos_frames = []
        data_group = file["data"]
        
        # å°è¯•ä»æ–‡ä»¶æ ¹çº§åˆ«æå–ä»»åŠ¡ä¿¡æ¯
        task_str = self._extract_task_info(file, task_name, file_path)
        
        # è·å–æ‰€æœ‰demo
        demo_keys = [k for k in data_group.keys() if k.startswith("demo_")]
        demo_keys.sort(key=lambda x: int(x.split("_")[1]))  # æŒ‰æ•°å­—æ’åº
        
        for demo_key in demo_keys:
            demo_frames = []
            demo_group = data_group[demo_key]
            
            # å°è¯•ä»demoçº§åˆ«æå–ä»»åŠ¡ä¿¡æ¯
            demo_task_str = self._extract_task_info(demo_group, task_str, file_path)
            
            # è¯»å–åŠ¨ä½œæ•°æ®
            actions = np.array(demo_group["actions"])
            
            # è¯»å–è§‚å¯Ÿæ•°æ®
            obs_group = demo_group["obs"]
            
            # è¯»å–å…³èŠ‚çŠ¶æ€ - ä½œä¸ºobservation.state
            joint_states = np.array(obs_group["joint_states"])
            
            # è¯»å–å›¾åƒæ•°æ®
            agentview_rgb = np.array(obs_group["agentview_rgb"])  # å‰è§†å›¾åƒ
            eye_in_hand_rgb = np.array(obs_group["eye_in_hand_rgb"])  # è…•éƒ¨å›¾åƒ
            
            # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
            num_frames = min(len(actions), len(joint_states), len(agentview_rgb), len(eye_in_hand_rgb))
            
            # å¤„ç†æ¯ä¸€å¸§
            for i in range(num_frames):
                # å¤„ç†å›¾åƒï¼šè°ƒæ•´å¤§å°åˆ°ç›®æ ‡å°ºå¯¸
                front_img = cv2.resize(agentview_rgb[i], (self.image_size[1], self.image_size[0]))
                wrist_img = cv2.resize(eye_in_hand_rgb[i], (self.image_size[1], self.image_size[0]))
                
                # ä¿®å¤å›¾åƒæ—‹è½¬é—®é¢˜ï¼šç¿»è½¬180åº¦
                front_img = cv2.flip(front_img, -1)  # -1è¡¨ç¤º180åº¦ç¿»è½¬
                wrist_img = cv2.flip(wrist_img, -1)  # -1è¡¨ç¤º180åº¦ç¿»è½¬
                
                # å‡†å¤‡å¸§æ•°æ® - å‚è€ƒRLDSæ ¼å¼
                frame_data = {
                    "task": demo_task_str,
                    "action": actions[i].astype(np.float32),
                    "observation.state": joint_states[i].astype(np.float32),
                    "observation.images.front": front_img,
                    "observation.images.wrist": wrist_img,
                }
                demo_frames.append(frame_data)
            
            demos_frames.append(demo_frames)
        
        return demos_frames

    def _extract_direct_episode_frames(self, file: h5py.File, task_name: str, file_path: Optional[Path] = None) -> List[List[Dict]]:
        """æå–ç›´æ¥episodeæ ¼å¼çš„å¸§æ•°æ®ï¼ˆå…œåº•æ–¹æ¡ˆï¼Œç”¨äºå¤šçº¿ç¨‹å¤„ç†ï¼‰- è¿”å›æŒ‰episodeåˆ†ç»„çš„æ•°æ®"""
        # å…œåº•æ–¹æ¡ˆï¼šå°†æ•´ä¸ªæ–‡ä»¶ä½œä¸ºä¸€ä¸ªepisode
        episode_frames = []
        
        # å°è¯•æå–ä»»åŠ¡ä¿¡æ¯
        task_str = self._extract_task_info(file, task_name, file_path)
        
        # å°è¯•æ‰¾åˆ°å¯èƒ½çš„çŠ¶æ€æ•°æ®
        state_keys = ["joint_states", "states", "robot_states", "state"]
        state_data = None
        
        for key in state_keys:
            if key in file:
                state_data = np.array(file[key])
                logger.info(f"æ‰¾åˆ°çŠ¶æ€æ•°æ®: {key}, shape: {state_data.shape}")
                break
        
        if state_data is None:
            raise KeyError("æœªæ‰¾åˆ°ä»»ä½•çŠ¶æ€æ•°æ®")
        
        # å°è¯•æ‰¾åˆ°åŠ¨ä½œæ•°æ®
        action_keys = ["actions", "action"]
        action_data = None
        
        for key in action_keys:
            if key in file:
                action_data = np.array(file[key])
                break
        
        if action_data is None:
            logger.warning("æœªæ‰¾åˆ°åŠ¨ä½œæ•°æ®ï¼Œä½¿ç”¨çŠ¶æ€æ•°æ®ä½œä¸ºåŠ¨ä½œ")
            action_data = state_data
        
        # å°è¯•æ‰¾åˆ°å›¾åƒæ•°æ®
        image_keys = ["agentview_rgb", "images", "rgb"]
        image_data = None
        
        for key in image_keys:
            if key in file:
                image_data = np.array(file[key])
                break
        
        if image_data is None:
            logger.warning("æœªæ‰¾åˆ°å›¾åƒæ•°æ®ï¼Œå°†åˆ›å»ºç©ºå›¾åƒ")
            image_data = np.zeros((len(state_data), *self.image_size, 3), dtype=np.uint8)
        
        # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
        num_frames = min(len(state_data), len(action_data), len(image_data))
        
        # å¤„ç†æ¯ä¸€å¸§
        for i in range(num_frames):
            # å¤„ç†å›¾åƒ
            if image_data.ndim == 4:  # æœ‰æ—¶é—´ç»´åº¦
                img = cv2.resize(image_data[i], (self.image_size[1], self.image_size[0]))
            else:  # æ²¡æœ‰æ—¶é—´ç»´åº¦ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ å›¾
                if len(image_data) > 0:
                    img = cv2.resize(image_data[0], 
                                   (self.image_size[1], self.image_size[0]))
                else:
                    img = np.zeros((*self.image_size, 3), dtype=np.uint8)
            
            # ä¿®å¤å›¾åƒæ—‹è½¬é—®é¢˜ï¼šç¿»è½¬180åº¦
            img = cv2.flip(img, -1)  # -1è¡¨ç¤º180åº¦ç¿»è½¬
            
            frame_data = {
                "task": task_str,
                "action": action_data[i].astype(np.float32),
                "observation.state": state_data[i].astype(np.float32),
                "observation.images.front": img,
                "observation.images.wrist": img,  # ä½¿ç”¨ç›¸åŒå›¾åƒä½œä¸ºè…•éƒ¨è§†å›¾
            }
            episode_frames.append(frame_data)
        
        return [episode_frames]  # è¿”å›å•ä¸ªepisodeçš„åˆ—è¡¨


class RLDSProcessor:
    """RLDSæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        if not HAS_TF:
            raise ImportError("tensorflow_datasetsæ˜¯RLDSå¤„ç†æ‰€å¿…éœ€çš„ï¼Œè¯·è¿è¡Œ: pip install tensorflow tensorflow_datasets")
        # æ·»åŠ ä¸HDF5Processorå…¼å®¹çš„å±æ€§
        self.image_size = (256, 256)
        self.use_videos = False
    
    def get_default_features(self, use_videos: bool = True) -> Dict[str, Dict[str, Any]]:
        """è·å–Liberoæ•°æ®é›†çš„é»˜è®¤ç‰¹å¾é…ç½®"""
        image_dtype = "video" if use_videos else "image"
        
        return {
            "observation.images.front": {
                "dtype": image_dtype,
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.images.wrist": {
                "dtype": image_dtype,
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.state": {
                "dtype": "float32",
                "shape": (8,),
                "names": [f"state_{i}" for i in range(8)],
            },
            "action": {
                "dtype": "float32",
                "shape": (7,),
                "names": [f"action_{i}" for i in range(7)],
            }
        }
    
    def process_dataset(self, dataset: LeRobotDataset, data_source: Union[str, Path]):
        """å¤„ç†RLDSæ•°æ®é›†"""
        # Liberoæ•°æ®é›†åç§°åˆ—è¡¨ã€‚æ ¹æ®è‡ªå·±éœ€æ±‚ä¿®æ”¹
        raw_dataset_names = [
            "libero_10_no_noops",
            "libero_goal_no_noops", 
            "libero_object_no_noops",
            "libero_spatial_no_noops",
        ]
        
        episode_idx = 0
        
        for raw_dataset_name in raw_dataset_names:
            logger.info(f"å¤„ç†RLDSæ•°æ®é›†: {raw_dataset_name}")
            
            try:
                # åŠ è½½RLDSæ•°æ®é›†
                raw_dataset = tfds.load(
                    raw_dataset_name, 
                    data_dir=data_source, 
                    split="train",
                    try_gcs=False
                )
                
                for episode in raw_dataset:
                    logger.info(f"å¤„ç†episode {episode_idx + 1}")
                    
                    # è·å–ä»»åŠ¡æè¿°
                    steps_list = list(episode["steps"].as_numpy_iterator())
                    task_str = f"episode_{episode_idx}"
                    
                    if steps_list and "language_instruction" in steps_list[0]:
                        task_str = steps_list[0]["language_instruction"].decode()
                    
                    # å¤„ç†episodeä¸­çš„æ¯ä¸ªstep
                    for step_idx, step in enumerate(steps_list):
                        frame_data = {
                            "task": task_str,
                            "observation.images.front": step["observation"]["image"],
                            "observation.images.wrist": step["observation"]["wrist_image"], 
                            "observation.state": step["observation"]["state"].astype(np.float32),
                            "action": step["action"].astype(np.float32),
                        }
                        # ä¿®å¤APIè°ƒç”¨
                        dataset.add_frame(frame_data)
                    
                    dataset.save_episode()
                    episode_idx += 1
                    
            except Exception as e:
                logger.warning(f"å¤„ç†æ•°æ®é›† {raw_dataset_name} æ—¶å‡ºé”™: {e}")
                continue


class UnifiedConverter:
    """ç»Ÿä¸€è½¬æ¢å™¨ç±»"""
    
    def __init__(self, num_workers: int = 4):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è½¬æ¢å™¨
        
        Args:
            num_workers: å¹¶è¡Œå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°
        """
        self.num_workers = num_workers
        self.detector = DatasetFormatDetector()
    
    def convert_dataset(
        self,
        data_dir: Union[str, Path],
        repo_id: str,
        output_dir: Optional[Union[str, Path]] = None,
        push_to_hub: bool = False,
        use_videos: bool = True,
        robot_type: str = "panda",
        fps: int = 20,
        task_name: str = "default_task",
        hub_config: Optional[Dict[str, Any]] = None,
        clean_existing: bool = True,
        image_writer_threads: int = 10,
        image_writer_processes: int = 5,
        run_compute_stats: bool = False,
        **kwargs
    ) -> LeRobotDataset:
        """
        ç»Ÿä¸€è½¬æ¢æ¥å£
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            repo_id: æ•°æ®é›†ä»“åº“ID
            output_dir: è¾“å‡ºç›®å½•
            push_to_hub: æ˜¯å¦æ¨é€åˆ°Hub
            use_videos: æ˜¯å¦ä½¿ç”¨è§†é¢‘æ ¼å¼
            robot_type: æœºå™¨äººç±»å‹
            fps: å¸§ç‡
            task_name: ä»»åŠ¡åç§°ï¼ˆHDF5æ ¼å¼ä½¿ç”¨ï¼‰
            hub_config: Hubé…ç½®
            clean_existing: æ˜¯å¦æ¸…ç†ç°æœ‰æ•°æ®é›†
            image_writer_threads: å›¾åƒå†™å…¥çº¿ç¨‹æ•°
            image_writer_processes: å›¾åƒå†™å…¥è¿›ç¨‹æ•°
            run_compute_stats: æ˜¯å¦è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            LeRobotDataset: è½¬æ¢åçš„æ•°æ®é›†
        """
        data_path = Path(data_dir)
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        format_type = self.detector.detect_format(data_path)
        logger.info(f"æ£€æµ‹åˆ°æ•°æ®æ ¼å¼: {format_type}")
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©å¤„ç†å™¨å’Œç‰¹å¾
        if format_type == "hdf5":
            processor = HDF5Processor()
            features = processor.get_default_features(use_videos)

        else:  # rlds
            processor = RLDSProcessor()
            features = processor.get_default_features(use_videos)
        
        # è®¾ç½®è¾“å‡ºè·¯å¾„
        if output_dir is None:
            lerobot_root = HF_LEROBOT_HOME
        else:
            lerobot_root = Path(output_dir)
        
        os.environ["LEROBOT_HOME"] = str(lerobot_root)
        lerobot_dataset_dir = lerobot_root / repo_id
        
        # æ¸…ç†ç°æœ‰æ•°æ®é›†
        if clean_existing and lerobot_dataset_dir.exists():
            logger.info(f"æ¸…ç†ç°æœ‰æ•°æ®é›†: {lerobot_dataset_dir}")
            shutil.rmtree(lerobot_dataset_dir)
        
        lerobot_root.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºLeRobotæ•°æ®é›†
        logger.info(f"åˆ›å»ºLeRobotæ•°æ®é›†: {repo_id}")
        dataset = LeRobotDataset.create(
            repo_id=repo_id,
            robot_type=robot_type,
            fps=fps,
            features=features,
            use_videos=use_videos,
            image_writer_processes=image_writer_processes,
            image_writer_threads=image_writer_threads,
        )
        
        # å¤„ç†æ•°æ®
        if format_type == "hdf5":
            self._process_hdf5_data(processor, dataset, data_path, task_name)
        else:  # rlds
            processor.process_dataset(dataset, data_path)
        
        # ç§»é™¤consolidateè°ƒç”¨ï¼Œå› ä¸ºAPIå¯èƒ½å·²æ›´æ”¹
        logger.info("æ•°æ®é›†å¤„ç†å®Œæˆ")
        
        # æ¨é€åˆ°Hub
        if push_to_hub:
            if hub_config is None:
                hub_config = self._get_default_hub_config()
            logger.info("æ¨é€åˆ°Hugging Face Hub...")
            dataset.push_to_hub(**hub_config)
        
        logger.info("âœ… æ•°æ®é›†è½¬æ¢å®Œæˆ!")
        return dataset
    
    def _process_hdf5_data(self, processor: Union[HDF5Processor, RLDSProcessor], dataset: LeRobotDataset, data_path: Path, task_name: str):
        """ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†HDF5æ•°æ®"""
        # ç¡®ä¿æ˜¯HDF5Processor
        if not isinstance(processor, HDF5Processor):
            raise TypeError("processorå¿…é¡»æ˜¯HDF5Processorå®ä¾‹")
            
        # æŸ¥æ‰¾æ‰€æœ‰episode
        episodes = []
        for ep_dir in data_path.iterdir():
            if ep_dir.is_dir():
                ep_path = ep_dir / "data" / "trajectory.hdf5"
                if ep_path.exists():
                    episodes.append(ep_path)
        
        if not episodes:
            # ç›´æ¥æŸ¥æ‰¾HDF5æ–‡ä»¶
            episodes = list(data_path.rglob("*.hdf5")) + list(data_path.rglob("*.h5"))
        
        logger.info(f"æ‰¾åˆ° {len(episodes)} ä¸ªepisodeæ–‡ä»¶")
        
        if self.num_workers == 1:
            # å•çº¿ç¨‹å¤„ç†
            for ep_path in tqdm(episodes, desc="å¤„ç†Episodes"):
                processor.process_episode(ep_path, dataset, task_name)
                logger.info(f"å¤„ç†å®Œæˆ: {ep_path.name}")
        else:
            # å¤šçº¿ç¨‹å¤„ç†
            self._process_episodes_parallel(processor, dataset, episodes, task_name)
    
    def _process_episodes_parallel(self, processor: HDF5Processor, dataset: LeRobotDataset, episodes: List[Path], task_name: str):
        """å¹¶è¡Œå¤„ç†episodesï¼Œä½¿ç”¨HDF5Processorçš„ç»Ÿä¸€æ–¹æ³•"""
        # åˆ›å»ºå¤„ç†å‡½æ•°
        def process_single_episode(ep_path: Path) -> Tuple[Path, bool, List[List[Dict]]]:
            """å¤„ç†å•ä¸ªepisodeå¹¶è¿”å›å¸§æ•°æ®"""
            demos_frames = []
            try:
                with h5py.File(ep_path, "r") as file:
                    logger.debug(f"å¤šçº¿ç¨‹å¤„ç†HDF5æ–‡ä»¶é”®: {list(file.keys())}")
                    
                    # ä½¿ç”¨ä¸HDF5Processorç›¸åŒçš„æ£€æµ‹å’Œå¤„ç†é€»è¾‘
                    if "data" in file:
                        # æ–°çš„Liberoæ ¼å¼ï¼šdata/demo_N/...
                        demos_frames = processor._extract_libero_demo_frames(file, task_name, ep_path)
                    else:
                        # å°è¯•å…¶ä»–æ ¼å¼çš„å…œåº•å¤„ç†
                        demos_frames = processor._extract_direct_episode_frames(file, task_name, ep_path)
                    
                return ep_path, True, demos_frames
            except Exception as e:
                logger.error(f"å¤„ç† {ep_path} å¤±è´¥: {e}")
                return ep_path, False, []
        
        # å¹¶è¡Œå¤„ç†
        logger.info(f"ä½¿ç”¨ {self.num_workers} ä¸ªå·¥ä½œçº¿ç¨‹å¹¶è¡Œå¤„ç†episodes")
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_episode = {executor.submit(process_single_episode, ep): ep for ep in episodes}
            
            # æ”¶é›†ç»“æœ
            for future in tqdm(concurrent.futures.as_completed(future_to_episode), total=len(episodes), desc="å¤„ç†Episodes"):
                ep_path, success, demos_frames_list = future.result()
                
                if success and demos_frames_list:
                    # æ¯ä¸ªdemoä½œä¸ºç‹¬ç«‹çš„episodeä¿å­˜
                    for demo_idx, demo_frames in enumerate(demos_frames_list):
                        for frame_data in demo_frames:
                            dataset.add_frame(frame_data)
                        dataset.save_episode()
                        logger.info(f"ä¿å­˜episode: {ep_path.name}_demo_{demo_idx}")
    
    def _get_default_hub_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤Hubé…ç½®"""
        return {
            "tags": ["libero", "robotics", "lerobot", "unified"],
            "private": False,
            "push_videos": True,
            "license": "apache-2.0",
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Liberoç»Ÿä¸€æ•°æ®è½¬æ¢å™¨ - æ”¯æŒRLDSå’ŒHDF5æ ¼å¼è‡ªåŠ¨è¯†åˆ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è½¬æ¢
  python unified_converter.py \\
    --data-dir /path/to/data \\
    --repo-id username/dataset_name \\
    --push-to-hub \\
    --use-videos \\
    --num-workers 4

  # HDF5æ ¼å¼ï¼ŒæŒ‡å®šé…ç½®æ–‡ä»¶
  python unified_converter.py \\
    --data-dir /path/to/hdf5/data \\
    --repo-id username/hdf5_dataset \\
    --config config.json \\
    --task-name "pick_and_place" \\
    --num-workers 8
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--data-dir", type=str, required=True, help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--repo-id", type=str, required=True, help="æ•°æ®é›†ä»“åº“ID")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output-dir", type=str, default=None, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--push-to-hub", action="store_true", help="æ¨é€åˆ°Hub")
    parser.add_argument("--private", action="store_true", help="åˆ›å»ºç§æœ‰æ•°æ®é›†")
    
    # æ•°æ®æ ¼å¼
    parser.add_argument("--use-videos", action="store_true", default=True, help="ä½¿ç”¨è§†é¢‘æ ¼å¼")
    parser.add_argument("--robot-type", type=str, default="panda", help="æœºå™¨äººç±»å‹")
    parser.add_argument("--fps", type=int, default=20, help="å¸§ç‡")
    
    # HDF5ç‰¹å®šå‚æ•°
    parser.add_argument("--task-name", type=str, default="default_task", help="ä»»åŠ¡åç§°")
    
    # æ€§èƒ½å‚æ•°
    parser.add_argument("--num-workers", type=int, default=2, help="å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--image-writer-processes", type=int, default=5, help="å›¾åƒå†™å…¥è¿›ç¨‹æ•°")
    parser.add_argument("--image-writer-threads", type=int, default=1, help="å›¾åƒå†™å…¥çº¿ç¨‹æ•°")
    
    # Hubé…ç½®
    parser.add_argument("--license", type=str, default="apache-2.0", help="æ•°æ®é›†è®¸å¯è¯")
    parser.add_argument("--tags", nargs="+", default=["libero", "robotics", "lerobot"], help="æ•°æ®é›†æ ‡ç­¾")
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†æ—¥å¿—")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # éªŒè¯å‚æ•°
    if not Path(args.data_dir).exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return 1
    
    if "/" not in args.repo_id:
        logger.error(f"repo_idæ ¼å¼é”™è¯¯: {args.repo_id}")
        return 1
    
    logger.info("ğŸ“‹ è½¬æ¢é…ç½®:")
    logger.info(f"  æ•°æ®æº: {args.data_dir}")
    logger.info(f"  ä»“åº“ID: {args.repo_id}")
    logger.info(f"  å¹¶è¡Œçº¿ç¨‹æ•°: {args.num_workers}")
    logger.info(f"  ä½¿ç”¨è§†é¢‘: {args.use_videos}")
    logger.info(f"  æ¨é€åˆ°Hub: {args.push_to_hub}")
    
    if args.dry_run:
        logger.info("âœ… è¯•è¿è¡Œå®Œæˆï¼Œå‚æ•°éªŒè¯é€šè¿‡")
        return 0
    
    # æ‰§è¡Œè½¬æ¢
    try:
        converter = UnifiedConverter(num_workers=args.num_workers)
        
        hub_config = {
            "tags": args.tags,
            "private": args.private,
            "license": args.license,
        }

        
        dataset = converter.convert_dataset(
            data_dir=args.data_dir,
            repo_id=args.repo_id,
            output_dir=args.output_dir,
            push_to_hub=args.push_to_hub,
            use_videos=args.use_videos,
            robot_type=args.robot_type,
            fps=args.fps,
            task_name=args.task_name,
            hub_config=hub_config,
            image_writer_processes=args.image_writer_processes,
            image_writer_threads=args.image_writer_threads,
        )
        
        logger.info("âœ… è½¬æ¢å®Œæˆ!")
        return 0
        
    except Exception as e:
        logger.error(f"è½¬æ¢å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main()) 
